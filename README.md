# Сервис мониторинга состояния складов

## Требования описаны в TASK_README.md

## Результаты
- Микросервис получает данные из kafka, записывает в БД Postgres, имеет две требуемые api ручки
- Кэширование реализовано на базе Redis. При записи указывается expire, при обновлении данных в БД соответствующий кэш инвалидируется
- Реализован один тест, проверяющий пошагово одно полное перемещение
- Нагрузочное тестирование было произведено с помощью Postman
- Мониторинг сервиса реализован с помощью [Prometheus + Grafana](http://localhost:3000). Готовый дашборд находится в dashboard.json. Локально будет нужно добавить data source (http://prometheus:9090), после чего импортировать файл
- OpenAPI спецификация доступна в файле `app/openapi.json`, а также [здесь](http://localhost:8000/docs)

## Решения относительно неясности в требованиях
- Количество товаров на складе не может быть ниже нуля, однако было принятно решение хранить в БД даже отрицательное количество и в api возвращать max(quantity, 0). Вполне возможна ситуация, что в этот сервис уже не придет какое-то очень старое сообщение, но так мы всегда корректно сохраняем разницу
- При запросе информации о перемещении разница в количестве товара является положительной, если было отправлено больше, чем пришло
- При запросе информации о перемещении разица по времени равна нулю, если перемещение неполное (только отбытие или только прибытие)
- При несуществовании склада или товара в нем api вернет количество товара 0

## Запуск
```
make start
```

Также реализована возможность запустить kafka producer (для нагрузочного тестирования)
```
make produce
```

Запуск теста
```
make test
```

## Возможные улучшения
- Разделить api и kafka consumer в отдельные микросервисы, т.к. это разные и независимые процессы. В текущем виде увеличение нагрузки на api может привести к замедлению получения сообщений из kafka и наоборот
- Добавить тестовые сценарии (неверные dto и тп)
- Изменить обработку сообщений в kafka consumer с поштучной на batch (собирать с валидацией N сообщений ИЛИ X времени, после чего обрабатывать их одновременно, не совершая штучные запросы к БД)

## Результаты нагрузочных тестирований
Во время всех запусков на фоне дополнительно работал producer для доп нагрузки за счет consumer'а. Error rate 50% из-за несуществования пемещений в базе (запросы были со случайными uuid, виртуально каждый раз сервису было необходимо обращаться напрямую к БД)

### 20 users
![](./load_test_20u.png?raw=true)

### 50 users
![](./load_test_50u.png?raw=true)

### 100 users
![](./load_test_100u.png?raw=true)
